# import the InferencePipeline interface
from inference import InferencePipeline
# import a built-in sink called render_boxes (sinks are the logic that happens after inference)
from inference.core.interfaces.stream.sinks import render_boxes

api_key = "YhsWQtGvkDwCH0br9fSA"

# create an inference pipeline object
pipeline = InferencePipeline.init(
    model_id="yolov8n-640", # set the model id to a yolov8n model with in put size 640
    video_reference="rtsp://admin:p@ssword@192.168.1.179:554/cam/realmonitor?channel=1&subtype=0", # set the video reference (source of video), it can be a link/path to a video file, an RTSP stream url, or an integer representing a device id (usually 0 for built in webcams)
    on_prediction=render_boxes, # tell the pipeline object what to do with each set of inference by passing a function
    api_key=api_key, # provide your roboflow api key for loading models from the roboflow api
)
# start the pipeline
pipeline.start()
# wait for the pipeline to finish
pipeline.join()
